{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Optimization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOI9G5mPiPRFzkF1VGhDpRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/novoforce/Exploring-Pytorch/blob/master/5_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zLaeCFLP09f"
      },
      "source": [
        "# Gradiant descent Update Rule\n",
        "\n",
        "# $w= w-\\eta \\frac{\\partial{L(w)}}{\\partial{w}}$  $\\Rightarrow [eq_1]$\n",
        "\n",
        "The $[eq_1]$ is the vanilla gradiant descent update rule.\n",
        "For optimization or better learning algorithms what all things need to be done:\n",
        "\n",
        "```\n",
        "How do you compute the gradient ?\n",
        "or What data should you use for computing the gradients ?\n",
        "```\n",
        "\n",
        "```\n",
        "How do you use the gradient ?\n",
        "or Can you come up with better update rule ?\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mcE4qyqhsPk"
      },
      "source": [
        "# Momentum based GD\n",
        "\n",
        "**Problems with GD:**\n",
        "If the loss curve(graph) is flat then the gradient update would be very very less which will eventually lead to huge training time as the algorithm will take longer time to get into the minima.\n",
        "\n",
        "So to mitigate the problems associated with Vanilla Gradient descent we are hereby proposing the Momentum based approach.\n",
        "\n",
        "**Intuition**\n",
        "\n",
        "If I'm repeatedly being asked to go in the same direction then I should probably gain some confidence(momentum) and start taking bigger steps in that direction. \n",
        "\n",
        "The below equation $[eq_2]$ and the $[eq_3]$ gives the final value for the momentum based GD.\n",
        "\n",
        "# $v_t=\\gamma.v_{t-1}+\\eta \\nabla w_t$  $\\Rightarrow[eq_2]$\n",
        "\n",
        "# $w_{t+1}=w_t-v_t$  $\\Rightarrow[eq_3]$\n",
        "\n",
        "Below image gives the mathematical expanation of the Momentum based GD.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/openscreenshot/j%2F8%2F1/gHJJUN18j.png\">\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "Going by the intuition. Suppose you are in the middle of the way and the you have asked people about which direction is the target and the values given by the people(or history) are represented as $v_0, v_1, v_2, v_3...v_t$\n",
        "\n",
        "So at timestep=0, history is $0$ (as no person encountered) thus $v_0=0$ \n",
        "\n",
        "At timestep=1,history is $\\eta \\nabla w_1$ thus $v_1= \\eta \\nabla w_1$\n",
        "\n",
        "At timestep=2,history is $\\gamma.\\eta \\nabla w_2+\\eta \\nabla w_2$ thus $v_2= \\gamma.\\eta \\nabla w_2+\\eta \\nabla w_2$\n",
        ".....\n",
        "\n",
        "One more important point to be noted is that after certain $n$ steps we will find that $\\gamma$ values are increasing and that represents the decay. Since the $\\gamma$ values are less than zero so the incresing the powers will result in the much lesser values.\n",
        "\n",
        "So intuitively speaking the if you are the current timestep $t$ and the previous person's information can be of less important as compared to the current person's information thus we are using the concept of decay to make the previous person's infor. less important.\n",
        "\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "Intuitively speaking,sometimes what happens is that we may farther distance because of the high confidence and then we have to turn back and come back.\n",
        "\n",
        "**So some of the interesting observations are:**\n",
        "\n",
        "Momentum based GD oscillates in-out of the minima valley, and despite this U-turns it still converges than the vanilla GD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm0ZL4OoPxTh"
      },
      "source": [
        "# Nesterov accelerated GD\n",
        "\n",
        "**Intuition:** Can we do something to reduce the Oscillations ?\n",
        "\n",
        "The answer is Nesterov accelerated GD which is basically 3 equations below:\n",
        "\n",
        "# $w_{temp}= w_t-\\gamma*v_{t-1} $    $\\Rightarrow[eq_4]$\n",
        "\n",
        "# $w_{t+1}= w_{temp}-\\eta \\nabla w_{temp}$   $\\Rightarrow[eq_5]$\n",
        "\n",
        "# $v_t= \\gamma*v_{t-1}+ \\eta \\nabla w_{temp}$   $\\Rightarrow[eq_6]$\n",
        "\n",
        "Before explaining Nesterov accelerated GD, let's see what the **Momentum based GD** does is that the algo. calculates the gradient before making a history update, whereas the **Nesterov accelarated GD(NAG)** makes the history update first then calculate the gradient.\n",
        "\n",
        "The new effect of this change is that even if the gradient might overshoot still the overshooting **won't be high** as that in the case of **Momentum based GD**.\n",
        "\n",
        "**Some Observations:**\n",
        "\n",
        "Look ahead helps NAG in correcting its course quicker than the momentum based GD. Hence the oscillations are smaller and thus less chance of escaping the valley lower.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktrz6zxNpYBj"
      },
      "source": [
        "# Concept of Adaptive Learning Rate\n",
        "\n",
        "Suppose in our dataset there may be certain features which are less in numbers as compared to other features, So to give more importance to those features we are using Adaptive learning rates which will increase the gradient values for low features and decrease gradient for high features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bstF7TSB_bwf"
      },
      "source": [
        "https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw0wDrQWPuzE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}