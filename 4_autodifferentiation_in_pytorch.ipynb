{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_autodifferentiation_in_pytorch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOs/6DLw5mU4orHWkgNGytN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/novoforce/Exploring-Pytorch/blob/master/4_autodifferentiation_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noGvYjCBCQSm"
      },
      "source": [
        "# Automatic differentiation mechanism\n",
        "[Autograd Pytorch](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
        "\n",
        "Neural networks usually rely on backpropagation to obtain gradients to update network parameters. The process of obtaining gradients is usually a very complicated and error-prone thing.\n",
        "\n",
        "The deep learning framework can help us automatically complete this gradient calculation.\n",
        "\n",
        "Pytorch generally uses the **backward method** to achieve this gradient calculation. **The gradient obtained by this method will be stored under the grad attribute of the corresponding independent variable tensor**.\n",
        "\n",
        "In addition, the **torch.autograd.grad** function can also be called to achieve gradient calculation.\n",
        "\n",
        "This is Pytorch's automatic differentiation mechanism.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u7W-ALYDdBO"
      },
      "source": [
        "## One, use the backward method to find the derivative\n",
        "\n",
        "The **backward method** is usually called on a scalar tensor, and the gradient obtained by this method will be stored under the **grad** property of the corresponding independent variable tensor.\n",
        "\n",
        "If the **called tensor is non-scalar**, a gradient parameter tensor with the same shape as it must be passed in.\n",
        "\n",
        "**It is equivalent to using the gradient parameter tensor and the calling tensor as a vector dot product, and the obtained scalar result is then backpropagated**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBrLV3nyFc7J"
      },
      "source": [
        "### 1, Backpropagation of scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPOcDPQzEwf6",
        "outputId": "c48d1839-bee4-4411-fd6b-fb2d6c3fd4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "\n",
        "# f(x) = a*x**2 + b*x + c derivative \n",
        "\n",
        "x = torch.tensor(0.0,requires_grad = True) # x needs to be derivated \n",
        "a = torch.tensor(1.0)\n",
        "b = torch.tensor(-2.0)\n",
        "c = torch.tensor(1.0)\n",
        "y = a*torch.pow(x,2) + b*x + c \n",
        "\n",
        "y.backward() #backward method\n",
        "dy_dx = x.grad\n",
        "print(dy_dx)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flIYT1HoFX4w"
      },
      "source": [
        "### 2, Non-scalar backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnrenO-jFMrx",
        "outputId": "d65af273-0ee6-4868-bb78-a258cbd96882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "\n",
        "# f(x) = a*x**2 + b*x + c\n",
        "\n",
        "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x needs to be derivated \n",
        "a = torch.tensor(1.0)\n",
        "b = torch.tensor(-2.0)\n",
        "c = torch.tensor(1.0)\n",
        "y = a*torch.pow(x,2) + b*x + c \n",
        "\n",
        "gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
        "\n",
        "print(\"x:\\n\",x)\n",
        "print(\"y:\\n\",y)\n",
        "y.backward(gradient = gradient)\n",
        "x_grad = x.grad\n",
        "print(\"x_grad:\\n\",x_grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            " tensor([[0., 0.],\n",
            "        [1., 2.]], requires_grad=True)\n",
            "y:\n",
            " tensor([[1., 1.],\n",
            "        [0., 1.]], grad_fn=<AddBackward0>)\n",
            "x_grad:\n",
            " tensor([[-2., -2.],\n",
            "        [ 0.,  2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeMdPQu_HQGe"
      },
      "source": [
        "### 3. Non-scalar backpropagation can be realized by scalar backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDMG1AYzFmHk",
        "outputId": "803d0b17-a89d-4483-97e7-f074deeb821d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "\n",
        "# f(x) = a*x**2 + b*x + c\n",
        "\n",
        "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True)\n",
        "a = torch.tensor(1.0)\n",
        "b = torch.tensor(-2.0)\n",
        "c = torch.tensor(1.0)\n",
        "y = a*torch.pow(x,2) + b*x + c \n",
        "\n",
        "gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
        "z = torch.sum(y*gradient)\n",
        "\n",
        "print(\"x:\",x)\n",
        "print(\"y:\",y)\n",
        "z.backward()\n",
        "x_grad = x.grad\n",
        "print(\"x_grad:\\n\",x_grad)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: tensor([[0., 0.],\n",
            "        [1., 2.]], requires_grad=True)\n",
            "y: tensor([[1., 1.],\n",
            "        [0., 1.]], grad_fn=<AddBackward0>)\n",
            "x_grad:\n",
            " tensor([[-2., -2.],\n",
            "        [ 0.,  2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWm_Nt4THxCm"
      },
      "source": [
        "## Second, use the autograd.grad method to find the derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1THreQDHqnj",
        "outputId": "f916c482-b31e-483a-f347-96ea39aba0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "\n",
        "# f(x) = a*x**2 + b*x + c derivative\n",
        "\n",
        "x = torch.tensor(0.0,requires_grad = True)\n",
        "a = torch.tensor(1.0)\n",
        "b = torch.tensor(-2.0)\n",
        "c = torch.tensor(1.0)\n",
        "y = a*torch.pow(x,2) + b*x + c\n",
        "\n",
        "\n",
        "# create_graph set to True will allow the creation of higher order derivatives \n",
        "dy_dx = torch.autograd.grad(y,x,create_graph=True)[0]\n",
        "print(dy_dx.data)\n",
        "\n",
        "# the second derivative\n",
        "dy2_dx2 = torch.autograd.grad(dy_dx,x)[0] \n",
        "\n",
        "print(dy2_dx2.data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-2.)\n",
            "tensor(2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2cUDYTgIFEO",
        "outputId": "9b934759-8830-42ef-9c7a-faad2d742c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "\n",
        "x1 = torch.tensor(1.0,requires_grad = True)\n",
        "x2 = torch.tensor(2.0,requires_grad = True)\n",
        "\n",
        "y1 = x1*x2\n",
        "y2 = x1+x2\n",
        "\n",
        "\n",
        "# allows simultaneous a plurality of independent variables the derivative \n",
        "(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = True)\n",
        "print(dy1_dx1,dy1_dx2)\n",
        "\n",
        "# If there are multiple dependent variables, it is equivalent to summing the gradient results of multiple dependent variables \n",
        "(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])\n",
        "print(dy12_dx1,dy12_dx2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.) tensor(1.)\n",
            "tensor(3.) tensor(2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHblg6NEJQFv"
      },
      "source": [
        "## Three, use automatic differentiation and optimizer to find the minimum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRQuQUqEItKb",
        "outputId": "7e5e5f81-4247-4608-cffa-2b57dce87ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "\n",
        "# f(x) = a*x**2 + b*x + c\n",
        "\n",
        "x = torch.tensor(0.0,requires_grad = True)\n",
        "a = torch.tensor(1.0)\n",
        "b = torch.tensor(-2.0)\n",
        "c = torch.tensor(1.0)\n",
        "\n",
        "optimizer = torch.optim.SGD(params=[x],lr = 0.01)\n",
        "\n",
        "\n",
        "def f(x):\n",
        "    result = a*torch.pow(x,2) + b*x + c \n",
        "    return(result)\n",
        "\n",
        "for i in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    y = f(x)\n",
        "    y.backward()\n",
        "    optimizer.step()\n",
        "   \n",
        "    \n",
        "print(\"y=\",f(x).data,\";\",\"x=\",x.data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y= tensor(0.) ; x= tensor(1.0000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}