{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1001_Pytorch_RNN_GRU_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0DtwmOk+I0pxeY3qcrDGs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/novoforce/Exploring-Pytorch/blob/master/new/1001_Pytorch_RNN_GRU_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m18JJ1PpkqZ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4qiyDjbqMJC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn #All neural network layers definitions are present\n",
        "import torch.nn.functional as F #functional api for NN layers, used to adding activations\n",
        "import torch.optim as optim #Optimizers are defined here\n",
        "from torch.utils.data import DataLoader #Data management for NN\n",
        "import torchvision.datasets as datasets #Datasets for the NN\n",
        "import torchvision.transforms as transforms #Data transform for augmentation"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAx3DzZjpkls"
      },
      "source": [
        "# Create a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj0hbD1SrWAv"
      },
      "source": [
        "class RNN(nn.Module):  #subclassing/inheriting from nn.Module\n",
        "  def __init__(self,input_size,hidden_size,num_layers,num_classes):  #input_size= 28x28 = 784\n",
        "    super(RNN,self).__init__() #calls the init function of the parent class(nn.Module)\n",
        "    self.hidden_size= hidden_size\n",
        "    self.num_layers= num_layers\n",
        "    self.rnn= nn.RNN(input_size,hidden_size,num_layers,batch_first=True)\n",
        "    self.fc= nn.Linear(hidden_size*sequence_length,num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x= F.relu(self.fc1(x)) #-->shape: (batch x 784)-->(batch x 50)\n",
        "    # x= self.fc2(x) #-->shape: (batch x 50)-->(batch x num_classes)    \n",
        "    h0= torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device) #initialize hidden state\n",
        "    # Forward Propagation\n",
        "    out,_ = self.rnn(x,h0)\n",
        "    out= out.reshape(out.shape[0],-1)\n",
        "    out= self.fc(out)\n",
        "    return out\n",
        "\n",
        "# Testing the class with some random generated values\n",
        "# model= RNN(28,256,2,10)\n",
        "# x= torch.randn((28,256,2,10))\n",
        "# print(model(x).shape)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMF4WtR0nXhk"
      },
      "source": [
        "# Create a Gated Recurrent Unit(GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjO3tyTznh4S"
      },
      "source": [
        "class GRU(nn.Module):  #subclassing/inheriting from nn.Module\n",
        "  def __init__(self,input_size,hidden_size,num_layers,num_classes):  #input_size= 28x28 = 784\n",
        "    super(GRU,self).__init__() #calls the init function of the parent class(nn.Module)\n",
        "    self.hidden_size= hidden_size\n",
        "    self.num_layers= num_layers\n",
        "    self.gru= nn.GRU(input_size,hidden_size,num_layers,batch_first=True)\n",
        "    self.fc= nn.Linear(hidden_size*sequence_length,num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x= F.relu(self.fc1(x)) #-->shape: (batch x 784)-->(batch x 50)\n",
        "    # x= self.fc2(x) #-->shape: (batch x 50)-->(batch x num_classes)    \n",
        "    h0= torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device) #initialize hidden state\n",
        "    # Forward Propagation\n",
        "    out,_ = self.gru(x,h0)\n",
        "    out= out.reshape(out.shape[0],-1)\n",
        "    out= self.fc(out)\n",
        "    return out\n",
        "\n",
        "# Testing the class with some random generated values\n",
        "# model= RNN(28,256,2,10)\n",
        "# x= torch.randn((28,256,2,10))\n",
        "# print(model(x).shape)\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYncmptpoHiB"
      },
      "source": [
        "# Create a Long Short Term Memory Unit(LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUyUu_j0oUhO"
      },
      "source": [
        "class LSTM(nn.Module):  #subclassing/inheriting from nn.Module\n",
        "  def __init__(self,input_size,hidden_size,num_layers,num_classes):  #input_size= 28x28 = 784\n",
        "    super(LSTM,self).__init__() #calls the init function of the parent class(nn.Module)\n",
        "    self.hidden_size= hidden_size\n",
        "    self.num_layers= num_layers\n",
        "    self.lstm= nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
        "    self.fc= nn.Linear(hidden_size*sequence_length,num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x= F.relu(self.fc1(x)) #-->shape: (batch x 784)-->(batch x 50)\n",
        "    # x= self.fc2(x) #-->shape: (batch x 50)-->(batch x num_classes)    \n",
        "    h0= torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device) #initialize hidden state\n",
        "    c0= torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device) #initialize cell state (specific for lstm)\n",
        "    # Forward Propagation\n",
        "    out,_ = self.lstm(x,(h0,c0))\n",
        "    out= out.reshape(out.shape[0],-1)\n",
        "    out= self.fc(out)\n",
        "    return out\n",
        "\n",
        "# Testing the class with some random generated values\n",
        "# model= RNN(28,256,2,10)\n",
        "# x= torch.randn((28,256,2,10))\n",
        "# print(model(x).shape)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGDL6LxSpkeR"
      },
      "source": [
        "# Set Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXBNl7suvIwz"
      },
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPWmdm91pkan"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2WgzTcavTn0"
      },
      "source": [
        "input_size= 28\n",
        "sequence_length= 28\n",
        "num_layers= 2\n",
        "hidden_size= 256\n",
        "num_classes= 10\n",
        "learning_rate= 0.001\n",
        "batch_size= 64\n",
        "num_epochs= 2"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esRLjL_UpkW7"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVJ8kgaYvlNV"
      },
      "source": [
        "train_dataset= datasets.MNIST(root='datasets/',train=True,transform=transforms.ToTensor(),download=True)\n",
        "train_loader= DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "test_dataset= datasets.MNIST(root='datasets/',train=False,transform=transforms.ToTensor(),download=True)\n",
        "test_loader= DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lros5ha1pkSF"
      },
      "source": [
        "# Initialize network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SysrFJq6wrCT"
      },
      "source": [
        "model= RNN(input_size,hidden_size,num_layers,num_classes).to(device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4sJop84nwI3"
      },
      "source": [
        "model= GRU(input_size,hidden_size,num_layers,num_classes).to(device)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXWXBIYcrmDY"
      },
      "source": [
        "model= LSTM(input_size,hidden_size,num_layers,num_classes).to(device)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmalpj8KpkLP"
      },
      "source": [
        "# Loss & Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_aDQH5Mw_NS"
      },
      "source": [
        "criterion= nn.CrossEntropyLoss()\n",
        "optimizer= optim.Adam(model.parameters(),lr=learning_rate) "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvda-Zo8pkCe"
      },
      "source": [
        "# Train Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQkF4qbbxUCo"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch_idx,(data,targets) in enumerate(train_loader):\n",
        "        data= data.to(device).squeeze(1)\n",
        "        targets= targets.to(device)\n",
        "        \n",
        "        #forward propagation\n",
        "        scores= model(data)\n",
        "        loss= criterion(scores,targets)\n",
        "\n",
        "        #backward propagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        #gradient descent\n",
        "        optimizer.step()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gGsbPODqCB5"
      },
      "source": [
        "# Check accuracy on training & test to see how good our model is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXwe-v_VnOov",
        "outputId": "abd6c24d-e97c-4d14-c398-5efa71dea45a"
      },
      "source": [
        "def check_accuracy(loader,model):\n",
        "    num_correct= 0\n",
        "    num_samples= 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if loader.dataset.train:\n",
        "            print(\"Checking accuracy on Training data\")\n",
        "        else:\n",
        "            print(\"Checking accuracy on Testing data\")\n",
        "\n",
        "        for x,y in loader:\n",
        "            x= x.to(device).squeeze(1)\n",
        "            y= y.to(device)\n",
        "            # print('y: ',y)\n",
        "            scores= model(x)  #batch x num_classes   Here 'model' is the trained\n",
        "            # print('scores: ',scores)\n",
        "            _,predictions= scores.max(1)\n",
        "            # print('vals: ',_,predictions)\n",
        "            # break\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f} %')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "check_accuracy(train_loader,model)\n",
        "check_accuracy(test_loader,model)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking accuracy on Training data\n",
            "Got 59116/60000 with accuracy 98.53 %\n",
            "Checking accuracy on Testing data\n",
            "Got 9827/10000 with accuracy 98.27 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TErDfxJ3C88"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}